{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alchemy API exercise\n",
    "\n",
    "- Team: [CognitiveBuild-TW](https://apps.na.collabserv.com/communities/service/html/communitystart?communityUuid=8e4d5ccf-5360-452b-8436-2fc1e649c348)\n",
    "- Author: Jesse Wei\n",
    "- Date: 2016/06/19\n",
    "\n",
    "\n",
    "<style>\n",
    "pre {\n",
    "    background-color: #f5f5f5;\n",
    "}\n",
    ".highlight {\n",
    "    background: #f8f8f8;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Alchemy Vision\n",
    "This is part of AlchemyAPI, other family member include Speech, Language, and Data Insights.\n",
    "\n",
    "### Scope\n",
    "* [Vision API](https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/alchemyvision/api/v1) : Uses deep learning innovations to understand a picture's content and context, to analyze an image and return information about the objects and people found within that image.\n",
    "* [Face Detection](https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/alchemyvision/api/v1/#imagegetrankedimagefacetags) : Tag faces in an image specified by URL or in the primary image in a web page specified by URL\n",
    "* [Image Link Extraction](https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/alchemyvision/api/v1/#urlgetimage) : Extract main image from posted HTML\n",
    "* [Image Tagging](https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/alchemyvision/api/v1/#urlgetrankedimagekeywords) : Tags an image specified by URL or the primary image in a web page specified by URL\n",
    "* Others..\n",
    "\n",
    "\n",
    "### API Reference:\n",
    "API 相關參考文件\n",
    "\n",
    "* [Alchemy@Bluemix](https://console.ng.bluemix.net/catalog/services/alchemyapi/)\n",
    "* [API Explorer](https://watson-api-explorer.mybluemix.net/apis/alchemy-vision-v1), [API doc](https://www.alchemyapi.com/api/keyword/htmlc.html), [API dev Doc](https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/visual-recognition.html)\n",
    "* [Demo@Bluemix](http://visual-recognition-demo.mybluemix.net/), [Demo@alchemy](http://vision.alchemy.ai/)\n",
    "* [SDK examples](https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py)\n",
    "* [SDK](https://github.com/watson-developer-cloud?utf8=%E2%9C%93&query=sdk): [Python](https://github.com/watson-developer-cloud/python-sdk), [Node.js](https://github.com/watson-developer-cloud/node-sdk)\n",
    "* [SDK visual recognition-nodejs](https://github.com/watson-developer-cloud/visual-recognition-nodejs)\n",
    "* [SDK visual insight-nodejs](https://github.com/watson-developer-cloud/visual-insights-nodejs)\n",
    "* **[This.API](https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py)**\n",
    "\n",
    "### This Notebook Resource: \n",
    "和本篇Notebook 有關的資源\n",
    "\n",
    "* [Community](https://apps.na.collabserv.com/communities/service/html/communitystart?communityUuid=8e4d5ccf-5360-452b-8436-2fc1e649c348)\n",
    "* [Docker](https://hub.docker.com/r/jessewei/jupyter_nodejs/)\n",
    "* Python learn[@Codecaemy](https://www.codecademy.com/learn/python),[@learnpython](http://www.learnpython.org/)\n",
    "* [Python Cheatsheet](http://www.astro.up.pt/~sousasag/Python_For_Astronomers/Python_qr.pdf)\n",
    "* [Markdown](https://help.github.com/articles/basic-writing-and-formatting-syntax/)\n",
    "* [FB Messager bot](http://tsaprailis.com/2016/06/02/How-to-build-and-deploy-a-Facebook-Messenger-bot-with-Python-and-Flask-a-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q&A **\n",
    "- Q: ** API-Face** issue, python return \"invalid-api-key-permissions\" why, but curl ok?\n",
    "    - ANS: VisionAPI is part of AlchemyAPI but undocument in bluemix now. Only found visual-recognition.\n",
    "- Q: ** API-Face** issue, no sample for nodejs/java/python from [API-doc](https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/visual-recognition/api/v3/?curl#detect_faces)\n",
    "    - ANS: There is a [github-nodejs sample](https://github.com/watson-developer-cloud/visual-recognition-nodejs) for visual-recognition\n",
    "- Q: ** API-Face** issue, change to [SDK visual-recognition examples](https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py) then OK.\n",
    "    - ANS: maybe version upgrade issue (06/07 updated), knew from empty sample in web document.\n",
    "- Q: ** API-Face** issue, resources location\n",
    "    - ANS: copy from exmples to relative nb directory then OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise purpose and procedure\n",
    "1. Change to your apikey, replace the myAPIkey and myURL to yours\n",
    "2. Follow steps to run and review\n",
    "3. Checkout API doc, write ourown samples\n",
    "4. Review visual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEMO Program Entry Point\n",
    "# 1. Change to your apikey\n",
    "\n",
    "myAPIkey = 'ChangeToYourKey'\n",
    "myVisionKey = 'ChangeToYourKey'\n",
    "myURL = 'https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg'\n",
    "# myURL = 'http://www.washingtonpost.com/blogs/capital-weather-gang/wp/2013/08/14/d-c-area-forecast-ultra-nice-weather-dominates-next-few-days/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"OK\", \n",
      "  \"usage\": \"By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html\", \n",
      "  \"NOTICE\": \"THIS API FUNCTIONALITY IS DEPRECATED AND HAS BEEN MIGRATED TO WATSON VISUAL RECOGNITION. THIS API WILL BE DISABLED ON MAY 19, 2017.\", \n",
      "  \"totalTransactions\": \"5\", \n",
      "  \"imageFaces\": [\n",
      "    {\n",
      "      \"positionX\": \"47\", \n",
      "      \"positionY\": \"45\", \n",
      "      \"gender\": {\n",
      "        \"gender\": \"MALE\", \n",
      "        \"score\": \"0.99593\"\n",
      "      }, \n",
      "      \"age\": {\n",
      "        \"ageRange\": \"45-54\", \n",
      "        \"score\": \"0.408353\"\n",
      "      }, \n",
      "      \"height\": \"103\", \n",
      "      \"width\": \"103\", \n",
      "      \"identity\": {\n",
      "        \"score\": \"0.982014\", \n",
      "        \"disambiguated\": {\n",
      "          \"website\": \"http://www.whitehouse.gov/\", \n",
      "          \"yago\": \"http://yago-knowledge.org/resource/Barack_Obama\", \n",
      "          \"name\": \"Barack Obama\", \n",
      "          \"freebase\": \"http://rdf.freebase.com/ns/m.02mjmr\", \n",
      "          \"subType\": [\n",
      "            \"Person\", \n",
      "            \"Politician\", \n",
      "            \"President\", \n",
      "            \"Appointer\", \n",
      "            \"AwardWinner\", \n",
      "            \"Celebrity\", \n",
      "            \"PoliticalAppointer\", \n",
      "            \"U.S.Congressperson\", \n",
      "            \"USPresident\", \n",
      "            \"TVActor\"\n",
      "          ], \n",
      "          \"dbpedia\": \"http://dbpedia.org/resource/Barack_Obama\"\n",
      "        }, \n",
      "        \"name\": \"Barack Obama\", \n",
      "        \"knowledgeGraph\": {\n",
      "          \"typeHierarchy\": \"/people/politicians/democrats/barack obama\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"status\": \"OK\", \n",
      "  \"NOTICE\": \"THIS API FUNCTIONALITY IS DEPRECATED AND HAS BEEN MIGRATED TO WATSON VISUAL RECOGNITION. THIS API WILL BE DISABLED ON MAY 19, 2017.\", \n",
      "  \"totalTransactions\": \"5\", \n",
      "  \"url\": \"https://upload.wikimedia.org/wikipedia/commons/9/9d/Barack_Obama.jpg\", \n",
      "  \"usage\": \"By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html\", \n",
      "  \"imageFaces\": [\n",
      "    {\n",
      "      \"positionX\": \"79\", \n",
      "      \"positionY\": \"105\", \n",
      "      \"gender\": {\n",
      "        \"gender\": \"MALE\", \n",
      "        \"score\": \"0.993307\"\n",
      "      }, \n",
      "      \"age\": {\n",
      "        \"ageRange\": \"35-44\", \n",
      "        \"score\": \"0.514129\"\n",
      "      }, \n",
      "      \"height\": \"260\", \n",
      "      \"width\": \"260\", \n",
      "      \"identity\": {\n",
      "        \"score\": \"0.880797\", \n",
      "        \"disambiguated\": {\n",
      "          \"website\": \"http://www.whitehouse.gov/\", \n",
      "          \"yago\": \"http://yago-knowledge.org/resource/Barack_Obama\", \n",
      "          \"name\": \"Barack Obama\", \n",
      "          \"freebase\": \"http://rdf.freebase.com/ns/m.02mjmr\", \n",
      "          \"subType\": [\n",
      "            \"Person\", \n",
      "            \"Politician\", \n",
      "            \"President\", \n",
      "            \"Appointer\", \n",
      "            \"AwardWinner\", \n",
      "            \"Celebrity\", \n",
      "            \"PoliticalAppointer\", \n",
      "            \"U.S.Congressperson\", \n",
      "            \"USPresident\", \n",
      "            \"TVActor\"\n",
      "          ], \n",
      "          \"dbpedia\": \"http://dbpedia.org/resource/Barack_Obama\"\n",
      "        }, \n",
      "        \"name\": \"Barack Obama\", \n",
      "        \"knowledgeGraph\": {\n",
      "          \"typeHierarchy\": \"/people/politicians/democrats/barack obama\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "ename": "WatsonException",
     "evalue": "Error: cannot-analyze:downstream-issue",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWatsonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-46bc04c429f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__file__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../resources/test.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     print(json.dumps(alchemy_vision.get_image_keywords(image_file, knowledge_graph=True,\n\u001b[1;32m---> 21\u001b[1;33m                                                        force_show_all=True), indent=2))\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# Text recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__file__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../resources/text.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/.pyenv/versions/anaconda-2.1.0/lib/python2.7/site-packages/watson_developer_cloud/alchemy_vision_v1.pyc\u001b[0m in \u001b[0;36mget_image_keywords\u001b[1;34m(self, image_file, image_url, knowledge_graph, force_show_all)\u001b[0m\n\u001b[0;32m     30\u001b[0m         params = {'knowledgeGraph': knowledge_graph,\n\u001b[0;32m     31\u001b[0m                   'forceShowAll': force_show_all}\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alchemy_image_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecognize_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknowledge_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/.pyenv/versions/anaconda-2.1.0/lib/python2.7/site-packages/watson_developer_cloud/watson_developer_cloud_service.pyc\u001b[0m in \u001b[0;36m_alchemy_image_request\u001b[1;34m(self, method_name, image_file, image_url, params)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         return self.request(method='POST', url=url, params=params, data=image_contents, headers=headers,\n\u001b[1;32m--> 207\u001b[1;33m                             accept_json=True)\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     def request(self, method, url, accept_json=False, headers=None, params=None, json=None, data=None, files=None,\n",
      "\u001b[1;32m/root/.pyenv/versions/anaconda-2.1.0/lib/python2.7/site-packages/watson_developer_cloud/watson_developer_cloud_service.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, accept_json, headers, params, json, data, files, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0merror_message\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'invalid-api-key'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mWatsonException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Error: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWatsonException\u001b[0m: Error: cannot-analyze:downstream-issue"
     ]
    }
   ],
   "source": [
    "# 2. Follow steps to run and review\n",
    "###  2.1. AlchemyAPI-Vision \n",
    "\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from watson_developer_cloud import AlchemyVisionV1\n",
    "\n",
    "alchemy_vision = AlchemyVisionV1(api_key=myVisionKey)\n",
    "\n",
    "\n",
    "# Face recognition\n",
    "with open(join(dirname('__file__'), '../resources/face.jpg'), 'rb') as image_file:\n",
    "    print(json.dumps(alchemy_vision.recognize_faces(image_file, knowledge_graph=True), indent=2))\n",
    "\n",
    "face_url = 'https://upload.wikimedia.org/wikipedia/commons/9/9d/Barack_Obama.jpg'\n",
    "print(json.dumps(alchemy_vision.recognize_faces(image_url=face_url, knowledge_graph=True), indent=2))\n",
    "\n",
    "# Image tagging\n",
    "with open(join(dirname('__file__'), '../resources/test.jpg'), 'rb') as image_file:\n",
    "    print(json.dumps(alchemy_vision.get_image_keywords(image_file, knowledge_graph=True,\n",
    "                                                       force_show_all=True), indent=2))\n",
    "# Text recognition\n",
    "with open(join(dirname('__file__'), '../resources/text.png'), 'rb') as image_file:\n",
    "    print(json.dumps(alchemy_vision.get_image_scene_text(image_file), indent=2))\n",
    "\n",
    "print(json.dumps(alchemy_vision.get_image_keywords(\n",
    "    image_url='https://upload.wikimedia.org/wikipedia/commons/8/81/Morris-Chair-Ironwood.jpg'), indent=2))\n",
    "\n",
    "# Image link extraction\n",
    "print(json.dumps(alchemy_vision.get_image_links(url='http://www.zillow.com/'), indent=2))\n",
    "\n",
    "with open(join(dirname('__file__'), '../resources/example.html'), 'r') as webpage:\n",
    "    print(json.dumps(alchemy_vision.get_image_links(html=webpage.read()), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"score\": 1.0, \n",
      "              \"class\": \"person\", \n",
      "              \"type_hierarchy\": \"/people\"\n",
      "            }\n",
      "          ], \n",
      "          \"classifier_id\": \"default\", \n",
      "          \"name\": \"default\"\n",
      "        }\n",
      "      ], \n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\", \n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ], \n",
      "  \"images_processed\": 1\n",
      "}\n",
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\", \n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\", \n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\", \n",
      "            \"score\": 0.5\n",
      "          }, \n",
      "          \"age\": {\n",
      "            \"max\": 54, \n",
      "            \"score\": 0.404682, \n",
      "            \"min\": 45\n",
      "          }, \n",
      "          \"face_location\": {\n",
      "            \"width\": 282, \n",
      "            \"top\": 131, \n",
      "            \"left\": 261, \n",
      "            \"height\": 296\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ], \n",
      "  \"images_processed\": 1\n",
      "}\n",
      "{\n",
      "  \"classifiers\": []\n",
      "}\n",
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"text\": \"its\\nopen\", \n",
      "      \"image\": \"../resources/text.png\", \n",
      "      \"words\": [\n",
      "        {\n",
      "          \"line_number\": 0, \n",
      "          \"score\": 0.972773, \n",
      "          \"word\": \"its\", \n",
      "          \"location\": {\n",
      "            \"width\": 79, \n",
      "            \"top\": 28, \n",
      "            \"left\": 22, \n",
      "            \"height\": 34\n",
      "          }\n",
      "        }, \n",
      "        {\n",
      "          \"line_number\": 1, \n",
      "          \"score\": 0.993081, \n",
      "          \"word\": \"open\", \n",
      "          \"location\": {\n",
      "            \"width\": 170, \n",
      "            \"top\": 68, \n",
      "            \"left\": 28, \n",
      "            \"height\": 68\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ], \n",
      "  \"images_processed\": 1\n",
      "}\n",
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"image\": \"../resources/face.jpg\", \n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\", \n",
      "            \"score\": 0.99593\n",
      "          }, \n",
      "          \"age\": {\n",
      "            \"max\": 54, \n",
      "            \"score\": 0.408353, \n",
      "            \"min\": 45\n",
      "          }, \n",
      "          \"identity\": {\n",
      "            \"score\": 0.982014, \n",
      "            \"name\": \"Barack Obama\", \n",
      "            \"type_hierarchy\": \"/people/politicians/democrats/barack obama\"\n",
      "          }, \n",
      "          \"face_location\": {\n",
      "            \"width\": 103, \n",
      "            \"top\": 45, \n",
      "            \"left\": 47, \n",
      "            \"height\": 109\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ], \n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 2. Follow steps to run and review\n",
    "###  2.2 visual-recognition\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from os import environ\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "\n",
    "test_url = myURL\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-05-20', api_key=myAPIkey)\n",
    "\n",
    "# with open(join(dirname(__file__), '../resources/cars.zip'), 'rb') as cars, \\\n",
    "#        open(join(dirname(__file__), '../resources/trucks.zip'), 'rb') as trucks:\n",
    "#     print(json.dumps(visual_recognition.create_classifier('Cars vs Trucks', cars_positive_examples=cars,\n",
    "#                                                           negative_examples=trucks), indent=2))\n",
    "\n",
    "# with open(join(dirname(__file__), '../resources/car.jpg'), 'rb') as image_file:\n",
    "#     print(json.dumps(visual_recognition.classify(images_file=image_file, threshold=0.1,\n",
    "#                                                  classifier_ids=['CarsvsTrucks_1675727418', 'default']), indent=2))\n",
    "\n",
    "# print(json.dumps(visual_recognition.get_classifier('YOUR CLASSIFIER ID'), indent=2))\n",
    "\n",
    "print(json.dumps(visual_recognition.classify(images_url=test_url), indent=2))\n",
    "\n",
    "print(json.dumps(visual_recognition.detect_faces(images_url=test_url), indent=2))\n",
    "\n",
    "# print(json.dumps(visual_recognition.delete_classifier(classifier_id='YOUR CLASSIFIER ID'), indent=2))\n",
    "\n",
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))\n",
    "\n",
    "with open(join(dirname('__file__'), '../resources/text.png'), 'rb') as image_file:\n",
    "    print(json.dumps(visual_recognition.recognize_text(images_file=image_file), indent=2))\n",
    "\n",
    "with open(join(dirname('__file__'), '../resources/face.jpg'), 'rb') as image_file:\n",
    "    print(json.dumps(visual_recognition.detect_faces(images_file=image_file), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The API\n",
    "'3. Checkout API doc, write our own samples\n",
    "\n",
    "### Face Detection  \n",
    "\n",
    "```\n",
    "alchemy_vision.recognize_faces(image_url=face_url, knowledge_graph=True))\n",
    "```\n",
    "**Keywords** - Face detection (Gender, age range, celebrity).\n",
    "\n",
    "\n",
    "### Image Link Extraction \n",
    "```\n",
    "alchemy_vision.get_image_links(url='http://www.zillow.com/')\n",
    "```\n",
    "\n",
    "**Image Link** - Extract main image from posted HTML.\n",
    "\n",
    "\n",
    "### Image Tagging\n",
    "```\n",
    "alchemy_vision.get_image_keywords(image_file, knowledge_graph=True, force_show_all=True)\n",
    "```\n",
    "\n",
    "**Image Tag** - Tags an image specified by URL or the primary image in a web page specified by URL.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n",
      "UNDER CONSTRUCTION\n"
     ]
    }
   ],
   "source": [
    "# Initaii plot \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n = 20\n",
    "X = np.arange(n)\n",
    "for x in X:\n",
    "    print 'UNDER CONSTRUCTION'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON process in Python\n",
    "\n",
    "print json.dumps(alchemy_language.keywords(5, url), 2)\n",
    "\n",
    "JSON to Python|Python to JSON\n",
    "--|--\n",
    "![json](json-j2p.jpg)|![json](json-p2j.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
